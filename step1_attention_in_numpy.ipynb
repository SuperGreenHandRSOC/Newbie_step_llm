{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20, 128) (10, 8, 20, 20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-143.92030352,  -30.61859787,  -75.5384955 ,  154.50956891,\n",
       "         -24.23250515,   42.76191746,   58.35442329,  168.83597452,\n",
       "         215.34228635,  321.11911673]),\n",
       " array([8.87333531e-059, 1.01785873e-132, 4.91141969e-049, 1.63399334e-103,\n",
       "        3.84843402e-084, 1.00000000e+000, 4.67253028e-114, 2.80462952e-085,\n",
       "        1.69103767e-140, 2.67974303e-186]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(114514)\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "  '''\n",
    "  1. 需要完成调整 K 的转置来匹配 Q 的最后一个维度，\n",
    "  2. 计算attn_score并缩放，\n",
    "  3. softmax 应用于最后一个轴计算attn_weight，\n",
    "  4. 应用attn_weights输出output\n",
    "  5. 带掩码mask的的注意力可以不用实现,但请记住encoder和decoder的transformer块是不一样的，很大一部分都在就在mask上\n",
    "  '''\n",
    "  K_T = np.swapaxes(K, -1, -2)\n",
    "  attention_weights = np.matmul(Q, K_T) / np.sqrt(K.shape[-1])\n",
    "  attention_weights = np.clip(attention_weights, -500, 500)\n",
    "  attention_weights = np.exp(attention_weights) / np.sum(np.exp(attention_weights), axis=-1, keepdims=True)\n",
    "  output = np.matmul(attention_weights, V)\n",
    "  return output, attention_weights\n",
    "\n",
    "def multi_head_attention(embed_size, num_heads, input, mask=None):\n",
    "  '''\n",
    "  1. embed_size 确保可以等分 num_heads 份， 否则输出错误\n",
    "  2. 随机初始化Wq,Wk,Wv,Wo矩阵，并对input做线性变换\n",
    "  3. 利用scaled_dot_product_attention()输出的attn_output计算O\n",
    "  4. 返回output, attN_weights\n",
    "  '''\n",
    "  assert embed_size % num_heads == 0\n",
    "  dk = int(embed_size / num_heads)\n",
    "  Wq = np.random.normal(0, 1, (num_heads, embed_size, dk))\n",
    "  Wk = np.random.normal(0, 1, (num_heads, embed_size, dk))\n",
    "  Wv = np.random.normal(0, 1, (num_heads, embed_size, dk))\n",
    "  Wo = np.random.normal(0, 1, (embed_size, embed_size))\n",
    "  q = np.matmul(input[:, np.newaxis, :, :], Wq)\n",
    "  k = np.matmul(input[:, np.newaxis, :, :], Wk)\n",
    "  v = np.matmul(input[:, np.newaxis, :, :], Wv)\n",
    "  output, weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "  output = tuple(np.moveaxis(output, 1, 0))\n",
    "  output = np.concatenate(output, axis=-1)\n",
    "  output = np.matmul(output, Wo)\n",
    "  \n",
    "  return output, weights\n",
    "\n",
    "# test e.g.\n",
    "embed_size = 128\n",
    "num_heads = 8\n",
    "input = np.random.randn(10, 20, embed_size)\n",
    "output, weights = multi_head_attention(embed_size, num_heads, input)\n",
    "print(output.shape, weights.shape)\n",
    "output[0][0][:10], weights[0][0][0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "   (10, 20, 128) (10, 8, 20, 20)\n",
    "   (array([-91.96555916, -19.40983534, -32.99740866, 113.35786088,\n",
    "           138.22610441,  81.21040905, -30.81003178,  90.7098463 ,\n",
    "           162.38724319, -40.72173619]),\n",
    "    array([1.94810489e-189, 3.21476597e-151, 3.61314239e-103, 4.96644350e-219,\n",
    "           3.90604112e-173, 3.46437823e-131, 4.72245009e-077, 2.66307289e-194,\n",
    "           1.00000000e+000, 5.17103825e-098]))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
